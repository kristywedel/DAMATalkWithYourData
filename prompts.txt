Prompt 1: app.py
Create app.py, a FastAPI-based product chatbot API that integrates a database search module and a locally hosted LLM. The API should:

Use FastAPI with CORS middleware to enable frontend interaction.
Expose endpoints:
GET / returns an HTML welcome page with API usage information.
POST /chat accepts a user query, redacts PII using Presidio, performs a database search (leveraging SQLite, FAISS, and relationship-based retrieval), and generates a response using an external LLM module.
Follow a modular architecture, using:
DatabaseSearch from database_search.py for SQLite, FAISS, and relationship-based retrieval.
LLMModel from llm_model.py for LLM processing, utilizing an open-source model for response generation.
Implement Pydantic models for request/response validation.
Run locally on 127.0.0.1:5000 using uvicorn, supporting live reload for development.
Ensure error handling for missing data, model failures, or unexpected inputs.
Follow best practices for maintainability and separation of concerns.

Prompt 2: Creating the Chat Interface (index.html)
Create index.html, a simple chat interface that:

Lets users ask queries via a chatbox.
Sends requests to http://127.0.0.1:5000/chat using fetch().
Displays chatbot responses dynamically.
Uses CSS for styling.
Maintains chat history.
Supports "Enter" key submission.
Handles errors if the API is unavailable.


Prompt 3: Validation Prompt
Create a validation script (validate_chatbot.py) that:

Ensures the chatbot API (app.py) runs correctly using FastAPI and Uvicorn.
Confirms FAISS is initialized properly and can return relevant products from data.json.
Validates SQLite (products.db) has expected tables (products and relationships) and is populated.
Checks if the LLM is loaded and responding.
Verifies /chat endpoint behavior by:
Sending test queries and confirming responses.
Ensuring FAISS returns semantic matches for vague queries.
Checking SQLite for exact lookup queries.
Running recursive SQL queries to validate relationship-based responses.
Ensuring the LLM generates a contextual response from aggregated data.
Validates session management by checking conversation_history.
Verifies CORS is enabled for frontend interaction.
Outputs a detailed validation report, logging errors if any steps fail.